#######################
training message:
- dataset: `sof-4423`
- split: pre-specified
#######################

### Fine-tuned GPT3 models and ChatGPT ###


01. curie:ft-personal-2023-05-30-03-33-28 [no-preprocess][special-prompt-v2]
testing result:
- Precision: [0.87165775 0.90368852 0.94181034]
- Recall:    [0.90555556 0.86811024 0.95414847]
- F-measure: [0.88828338 0.88554217 0.94793926]
- Accuracy:  0.9079939668174962
- Classification Report:
              precision    recall  f1-score   support

    negative     0.8717    0.9056    0.8883       360
     neutral     0.9037    0.8681    0.8855       508
    positive     0.9418    0.9541    0.9479       458

    accuracy                         0.9080      1326
   macro avg     0.9057    0.9093    0.9073      1326
weighted avg     0.9082    0.9080    0.9078      1326


02. babbage:ft-personal-2023-05-29-18-51-23 [no-preprocess][special-prompt]
testing result:
- Precision: [0.88010899 0.88554217 0.94143167]
- Recall:    [0.89722222 0.86811024 0.94759825]
- F-measure: [0.88858322 0.87673956 0.9445049 ]
- Accuracy:  0.9034690799396682
- Classification Report:
              precision    recall  f1-score   support

    negative     0.8801    0.8972    0.8886       360
     neutral     0.8855    0.8681    0.8767       508
    positive     0.9414    0.9476    0.9445       458

    accuracy                         0.9035      1326
   macro avg     0.9024    0.9043    0.9033      1326
weighted avg     0.9034    0.9035    0.9034      1326


03. babbage:ft-personal-2023-05-29-18-11-55 [no-preprocess][special-prompt-v2]
testing result:
- Precision: [0.8746594  0.89409369 0.93589744]
- Recall:    [0.89166667 0.86417323 0.95633188]
- F-measure: [0.88308116 0.87887888 0.94600432]
- Accuracy:  0.9034690799396682
- Classification Report:
              precision    recall  f1-score   support

    negative     0.8747    0.8917    0.8831       360
     neutral     0.8941    0.8642    0.8789       508
    positive     0.9359    0.9563    0.9460       458

    accuracy                         0.9035      1326
   macro avg     0.9016    0.9041    0.9027      1326
weighted avg     0.9033    0.9035    0.9032      1326


04. ada:ft-personal-2023-05-30-05-21-25 [no-preprocess][special-prompt-v2][augmented]
testing result:
- Precision: [0.89337176 0.875      0.93576017]
- Recall:    [0.86111111 0.88188976 0.95414847]
- F-measure: [0.87694484 0.87843137 0.94486486]
- Accuracy:  0.9012066365007542
- Classification Report:
              precision    recall  f1-score   support

    negative     0.8934    0.8611    0.8769       360
     neutral     0.8750    0.8819    0.8784       508
    positive     0.9358    0.9541    0.9449       458

    accuracy                         0.9012      1326
   macro avg     0.9014    0.8990    0.9001      1326
weighted avg     0.9010    0.9012    0.9010      1326


05. ada:ft-personal-2023-05-24-16-11-39 [no-preprocess][special-prompt]
testing result:
- Precision: [0.87988827 0.87872763 0.93978495]
- Recall:    [0.875      0.87007874 0.95414847]
- F-measure: [0.87743733 0.8743818  0.94691224]
- Accuracy:  0.9004524886877828
- Classification Report:
              precision    recall  f1-score   support

    negative     0.8799    0.8750    0.8774       360
     neutral     0.8787    0.8701    0.8744       508
    positive     0.9398    0.9541    0.9469       458

    accuracy                         0.9005      1326
   macro avg     0.8995    0.8997    0.8996      1326
weighted avg     0.9001    0.9005    0.9003      1326


06. ada:ft-personal-2023-05-29-18-08-41 [no-preprocess][special-prompt-v2]
testing result:
- Precision: [0.86178862 0.87848606 0.94945055]
- Recall:    [0.88333333 0.86811024 0.94323144]
- F-measure: [0.87242798 0.87326733 0.94633078]
- Accuracy:  0.8981900452488688
- Classification Report:
              precision    recall  f1-score   support

    negative     0.8618    0.8833    0.8724       360
     neutral     0.8785    0.8681    0.8733       508
    positive     0.9495    0.9432    0.9463       458

    accuracy                         0.8982      1326
   macro avg     0.8966    0.8982    0.8973      1326
weighted avg     0.8985    0.8982    0.8983      1326


07. ada:ft-personal-2023-05-29-12-45-33 [no-preprocess][special-prompt][validate]
testing result:
- Precision: [0.88418079 0.86381323 0.94541485]
- Recall:    [0.86944444 0.87401575 0.94541485]
- F-measure: [0.8767507  0.86888454 0.94541485]
- Accuracy:  0.8974358974358975
- Classification Report:
              precision    recall  f1-score   support

    negative     0.8842    0.8694    0.8768       360
     neutral     0.8638    0.8740    0.8689       508
    positive     0.9454    0.9454    0.9454       458

    accuracy                         0.8974      1326
   macro avg     0.8978    0.8963    0.8970      1326
weighted avg     0.8975    0.8974    0.8975      1326


08. ada:ft-personal-2023-05-24-14-29-32 [no-preprocess]
testing result:
- Precision: [0.8611898  0.80036298 0.9478673 ]
- Recall:    [0.84444444 0.86811024 0.87336245]
- F-measure: [0.85273492 0.83286119 0.90909091]
- Accuracy:  0.8634992458521871
- Classification Report:
              precision    recall  f1-score   support

    negative     0.8612    0.8444    0.8527       360
     neutral     0.8004    0.8681    0.8329       508
    positive     0.9479    0.8734    0.9091       458

    accuracy                         0.8635      1326
   macro avg     0.8698    0.8620    0.8649      1326
weighted avg     0.8678    0.8635    0.8646      1326


09. gpt3.5-turbo [no-preprocess]
testing result:
- Precision: [0.79069767 0.6404321  0.90718563]
- Recall:    [0.75555556 0.81692913 0.66157205]
- F-measure: [0.77272727 0.71799308 0.76515152]
- Accuracy:  0.746606334841629
- Classification Report:
              precision    recall  f1-score   support

    negative     0.7907    0.7556    0.7727       360
     neutral     0.6404    0.8169    0.7180       508
    positive     0.9072    0.6616    0.7652       458

    accuracy                         0.7466      1326
   macro avg     0.7794    0.7447    0.7520      1326
weighted avg     0.7734    0.7466    0.7491      1326
